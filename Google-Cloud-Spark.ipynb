{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "672fafab-c21a-4126-86b7-025723c2f682",
   "metadata": {},
   "source": [
    "# TAREA 3: SERGIO SOLER ROCHA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e96019-9aff-40ff-a8cb-cfbe79b887e8",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "El objetivo de este ejercicio es implementar un benchmark en Google Cloud Platform (GCP) para evaluar el desempeño de un despliegue basado en Spark, centrándose en velocidad, rendimiento y uso de recursos. Se busca analizar de un cluster en GCP, utilizando servicios  como Dataproc (para ejecución de clústeres de Apache Spark y Apache Hadoop) y Google Cloud Storage (para almacenamiento en la nube). El benchmark permitirá extraer datos y realizar análisis para comprender las implicaciones del rendimiento en diferentes escenarios, facilitando la toma de decisiones sobre la configuración óptima y la mejora continua de la infraestructura en la nube.\n",
    "\n",
    "El benchmark utilizado es se encuentra en la siguiente url: github.com/DIYBigData/pyspark-benchmark/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc5c3a4-2fae-4b85-96bd-d3184625b611",
   "metadata": {},
   "source": [
    "## Configuración inicial en GCP\n",
    "Creamos un clúster implementado en Google Cloud Platform con Dataproc, denominado \"nombre-del-cluster\". A continuación, se detallan las principales características del clúster:\n",
    "\n",
    "- Nombre del Clúster: nombre-del-cluster\n",
    "- Tipo de Cluster: Standard\n",
    "- Estado: En ejecución\n",
    "- Región: europe-west6\n",
    "- Zona: europe-west6-c\n",
    "- Total de Nodos Trabajadores: 2\n",
    "- VMs Flexibles: No\n",
    "- Eliminación Programada: Desactivado\n",
    "- Bucket de Etapa de Pruebas de Cloud Storage: dataproc-staging-europe-west6-592252024140-2eafoyog.¡\n",
    "\n",
    "Lo configuramos con dos nodos trabajadores para distribuir eficientemente las tareas de procesamiento. La desactivación de la eliminación programada garantiza la persistencia del clúster, mientras que el almacenamiento en el bucket de etapa de pruebas de Cloud Storage facilita el manejo de datos durante el proceso de ejecución.\n",
    "\n",
    "Ahora creamos un Bucket en Google Cloud Storage. Este Bucket será destinado para almacenar los datos y los scripts esenciales que serán utilizados en las evaluaciones de rendimiento, es decir, los script generate-data.py, benchmark-shuffle.py y benchmark-cpu.py.\n",
    "\n",
    "El siguiente paso es Instalar la biblioteca de Google Cloud Storage para Python, desde el cmd escribimos pip install google-cloud-storage. La instalación de la biblioteca de Google Cloud Storage para Python es fundamental para habilitar la interacción y manipulación de objetos almacenados en Google Cloud Storage (GCS) a través de scripts y aplicaciones escritas en Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ad78d-fde9-4268-9e8e-5feb892b20fe",
   "metadata": {},
   "source": [
    "## Preparación de Datos\n",
    "\n",
    "Para llevar a cabo las pruebas de rendimiento, se utiliza el script generate-data.py para generar conjuntos de datos de prueba, el cual es un trabajo PySpark. El archivo resultante será un archivo CSV particionado. Estos son los siguientes parámetros:\n",
    "\n",
    "- --master spark://spark-master:7077: Opción de spark-submit que identifica la ubicación del maestro de Spark.\n",
    "- --name 'generate-benchmark-test-data': Opción de spark-submit para nombrar el trabajo presentado (opcional- ).\n",
    "- /path/to/test/data/file: Ruta completa donde se generará el archivo de datos de prueba. \n",
    "- -r num_rows: Número total de filas a generar. Cada fila tiene aproximadamente 75 bytes.\n",
    "- -p num_partitions: Número de particiones que debería tener el archivo de datos de p\n",
    "\n",
    "El esquema de los datos producidos por el script es el siguiente:\n",
    "\n",
    "root\n",
    "- |-- value: string (nullable = true)\n",
    "- |-- prefix2: string (nullable = true)\n",
    "- |-- prefix4: string (nullable = true)\n",
    "- |-- prefix8: string (nullable = true)\n",
    "- |-- float_val: string (nullable = true)\n",
    "- |-- integer_val: string (nullable = true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce99fe-6a2a-4998-849d-3d84365774ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos login con nuestra cuenta de Google Cloud\n",
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40299ddd-a4e8-4b44-9b0a-7db7ca0528dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "# Establecemos el proyecto predeterminado en el entorno actual de Google Cloud SDK\n",
    "!gcloud config set project practica-3-411314\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa4a0d-bbcf-41f7-b103-d908ced30ec5",
   "metadata": {},
   "source": [
    "La siguiente línea de código envía un trabajo PySpark al clúster de Dataproc con el script \"generate-data.py\", generando 1000000 filas de datos particionadas en 100 particiones y guardando el resultado en un archivo CSV en el bucket creado anteriormente de Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a9ff478-dc7a-4eae-a60d-871544667835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: true\n",
      "driverControlFilesUri: gs://dataproc-staging-europe-west6-592252024140-2eafoyog/google-cloud-dataproc-metainfo/33b9567d-8933-4b45-b085-d8e6f9dd801d/jobs/092c8be3b8ab46849c8f4acd700d5b40/\n",
      "driverOutputResourceUri: gs://dataproc-staging-europe-west6-592252024140-2eafoyog/google-cloud-dataproc-metainfo/33b9567d-8933-4b45-b085-d8e6f9dd801d/jobs/092c8be3b8ab46849c8f4acd700d5b40/driveroutput\n",
      "jobUuid: 811f1614-a002-3353-a983-1fd76ea2df94\n",
      "placement:\n",
      "  clusterName: nombre-del-cluster\n",
      "  clusterUuid: 33b9567d-8933-4b45-b085-d8e6f9dd801d\n",
      "pysparkJob:\n",
      "  args:\n",
      "  - gs://bucket_1990/Data/datos_generados.csv\n",
      "  - -r\n",
      "  - '1000000'\n",
      "  - -p\n",
      "  - '100'\n",
      "  mainPythonFileUri: gs://bucket_1990/generate-data.py\n",
      "reference:\n",
      "  jobId: 092c8be3b8ab46849c8f4acd700d5b40\n",
      "  projectId: practica-3-411314\n",
      "status:\n",
      "  state: DONE\n",
      "  stateStartTime: '2024-01-19T11:35:58.777993Z'\n",
      "statusHistory:\n",
      "- state: PENDING\n",
      "  stateStartTime: '2024-01-19T11:35:05.915519Z'\n",
      "- state: SETUP_DONE\n",
      "  stateStartTime: '2024-01-19T11:35:05.961196Z'\n",
      "- details: Agent reported job success\n",
      "  state: RUNNING\n",
      "  stateStartTime: '2024-01-19T11:35:06.251834Z'\n",
      "yarnApplications:\n",
      "- name: generate-data.py\n",
      "  progress: 1.0\n",
      "  state: FINISHED\n",
      "  trackingUrl: http://nombre-del-cluster-m:8088/proxy/application_1705477825542_0005/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [092c8be3b8ab46849c8f4acd700d5b40] submitted.\n",
      "Waiting for job output...\n",
      "24/01/19 11:35:11 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "24/01/19 11:35:11 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "24/01/19 11:35:11 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/01/19 11:35:11 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "24/01/19 11:35:11 INFO org.sparkproject.jetty.util.log: Logging initialized @4064ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "24/01/19 11:35:12 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_392-b08\n",
      "24/01/19 11:35:12 INFO org.sparkproject.jetty.server.Server: Started @4183ms\n",
      "24/01/19 11:35:12 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@53a11f40{HTTP/1.1, (http/1.1)}{0.0.0.0:37771}\n",
      "24/01/19 11:35:12 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at nombre-del-cluster-m/10.172.0.3:8032\n",
      "24/01/19 11:35:13 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at nombre-del-cluster-m/10.172.0.3:10200\n",
      "24/01/19 11:35:14 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\n",
      "24/01/19 11:35:14 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "24/01/19 11:35:15 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1705477825542_0005\n",
      "24/01/19 11:35:16 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at nombre-del-cluster-m/10.172.0.3:8030\n",
      "24/01/19 11:35:18 INFO com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=226; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-europe-west6-592252024140-wiwqtlkk/33b9567d-8933-4b45-b085-d8e6f9dd801d/spark-job-history\n",
      "24/01/19 11:35:18 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
      "24/01/19 11:35:18 INFO com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=171; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-europe-west6-592252024140-wiwqtlkk/33b9567d-8933-4b45-b085-d8e6f9dd801d/spark-job-history\n",
      "24/01/19 11:35:19 INFO __main__: ****************************************************************\n",
      "24/01/19 11:35:19 INFO __main__: \n",
      "24/01/19 11:35:19 INFO __main__: Starting creation of test data file with 1000000 rows and 100 partitions at gs://bucket_1990/Data/datos_generados.csv\n",
      "24/01/19 11:35:19 INFO __main__: \n",
      "24/01/19 11:35:19 INFO __main__: ****************************************************************\n",
      "24/01/19 11:35:24 INFO com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_delete. latencyMs=560; previousMaxLatencyMs=0; operationCount=1; context=gs://bucket_1990/Data/datos_generados.csv\n",
      "24/01/19 11:35:25 INFO com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=213; previousMaxLatencyMs=171; operationCount=2; context=gs://bucket_1990/Data/datos_generados.csv/_temporary/0\n",
      "24/01/19 11:35:56 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://bucket_1990/Data/datos_generados.csv/' directory.\n",
      "24/01/19 11:35:56 INFO com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=231; previousMaxLatencyMs=0; operationCount=1; context=gs://bucket_1990/Data/datos_generados.csv/_SUCCESS\n",
      "24/01/19 11:35:56 INFO __main__: Done writing to gs://bucket_1990/Data/datos_generados.csv\n",
      "24/01/19 11:35:56 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@53a11f40{HTTP/1.1, (http/1.1)}{0.0.0.0:0}\n",
      "Job [092c8be3b8ab46849c8f4acd700d5b40] finished successfully.\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc jobs submit pyspark \\\n",
    "  --cluster=nombre-del-cluster \\\n",
    "  --region=europe-west6 \\\n",
    "  gs://bucket_1990/generate-data.py \\\n",
    "  -- gs://bucket_1990/Data/datos_generados.csv -r 1000000 -p 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5d901e-0ffd-4dac-8d97-d1566057fc4d",
   "metadata": {},
   "source": [
    "## Ejecución del Benchmark en Dataproc: benchmark-shuffle.py\n",
    "El script benchmark-shuffle.py se centra en evaluar el rendimiento de operaciones comunes de PySpark en marcos de datos que desencadenan una operación de \"shuffle\". Las operaciones de prueba incluyen:\n",
    "\n",
    "- Group By and Aggregate: Agrupación y agregación de datos.\n",
    "- Repartition: Reorganización de datos entre particiones.\n",
    "- Inner Join: Operación de unión interna entre dos conjuntos de datos.\n",
    "- Broadcast Inner Join: Unión interna con transmisión de datos pequeños.\n",
    "\n",
    "A continuación ejecutamos el script benchmark-shuffle.py sobre los datos generados en el apartado anterior. El número de particiones que se utilizarán durante la prueba de repartición será 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f738c5b-3d31-42f1-a9c4-2066031448ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: true"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [c18ad85312d441ba9d175d9ea1cbe3a9] submitted.\n",
      "Waiting for job output...\n",
      "24/01/19 11:36:54 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "24/01/19 11:36:54 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "24/01/19 11:36:54 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/01/19 11:36:54 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "24/01/19 11:36:54 INFO org.sparkproject.jetty.util.log: Logging initialized @3835ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "24/01/19 11:36:54 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_392-b08\n",
      "24/01/19 11:36:54 INFO org.sparkproject.jetty.server.Server: Started @3975ms\n",
      "24/01/19 11:36:54 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@461e3540{HTTP/1.1, (http/1.1)}{0.0.0.0:41809}\n",
      "24/01/19 11:36:55 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at nombre-del-cluster-m/10.172.0.3:8032\n",
      "24/01/19 11:36:55 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at nombre-del-cluster-m/10.172.0.3:10200\n",
      "24/01/19 11:36:57 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\n",
      "24/01/19 11:36:57 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "24/01/19 11:36:57 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1705477825542_0006\n",
      "24/01/19 11:36:58 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at nombre-del-cluster-m/10.172.0.3:8030\n",
      "24/01/19 11:37:00 INFO com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=268; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-europe-west6-592252024140-wiwqtlkk/33b9567d-8933-4b45-b085-d8e6f9dd801d/spark-job-history\n",
      "24/01/19 11:37:01 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
      "24/01/19 11:37:01 INFO com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=162; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-europe-west6-592252024140-wiwqtlkk/33b9567d-8933-4b45-b085-d8e6f9dd801d/spark-job-history\n",
      "24/01/19 11:37:02 INFO __main__: **********************************************************************\n",
      "24/01/19 11:37:02 INFO __main__: \n",
      "24/01/19 11:37:02 INFO __main__: Benchmarking PySpark's shuffle capacity using input data at gs://bucket_1990/Data/datos_generados.csv\n",
      "24/01/19 11:37:02 INFO __main__: \n",
      "24/01/19 11:37:02 INFO __main__: **********************************************************************\n",
      "24/01/19 11:37:05 INFO __main__: ****************************************************************\n",
      "24/01/19 11:37:05 INFO __main__: Starting bench mark test for Group By\n",
      "24/01/19 11:37:20 INFO __main__: The count value for the groupBy benchmark is = 256\n",
      "24/01/19 11:37:20 INFO __main__: \n",
      "24/01/19 11:37:20 INFO __main__: ****************************************************************\n",
      "24/01/19 11:37:20 INFO __main__: Starting bench mark test for Repartition\n",
      "24/01/19 11:37:29 INFO __main__: The count value for the repartition benchmark is = 1000000\n",
      "24/01/19 11:37:29 INFO __main__: The number of partitions after the repartition benchmark is = 200\n",
      "24/01/19 11:37:29 INFO __main__: \n",
      "24/01/19 11:37:29 INFO __main__: ****************************************************************\n",
      "24/01/19 11:37:29 INFO __main__: Starting bench mark test for Inner Join\n",
      "24/01/19 11:37:43 INFO __main__: The count value for the inner join benchmark is = 1000000\n",
      "24/01/19 11:37:43 INFO __main__: \n",
      "24/01/19 11:37:43 INFO __main__: ****************************************************************\n",
      "24/01/19 11:37:43 INFO __main__: Starting bench mark test for Broadcast Inner Join\n",
      "24/01/19 11:37:50 INFO __main__: The count value for the broadcast inner join benchmark is = 1000000\n",
      "24/01/19 11:37:50 INFO __main__: \n",
      "24/01/19 11:37:50 INFO __main__: **********************************************************************\n",
      "24/01/19 11:37:50 INFO __main__:     RESULTS    RESULTS    RESULTS    RESULTS    RESULTS    RESULTS\n",
      "24/01/19 11:37:50 INFO __main__:     Test Run = 'shuffle-benchmark'\n",
      "24/01/19 11:37:50 INFO __main__: \n",
      "24/01/19 11:37:50 INFO __main__: Group By test time         = 15.213665729999775 seconds\n",
      "24/01/19 11:37:50 INFO __main__: Repartition test time      = 8.370773898001062 seconds (200 partitions)\n",
      "24/01/19 11:37:50 INFO __main__: Inner join test time       = 14.873300867009675 seconds \n",
      "24/01/19 11:37:50 INFO __main__: Broadcast inner join time  = 6.686923230998218 seconds \n",
      "24/01/19 11:37:50 INFO __main__: \n",
      "24/01/19 11:37:50 INFO __main__: **********************************************************************\n",
      "24/01/19 11:37:50 INFO __main__: \n",
      "24/01/19 11:37:50 INFO __main__: Writing results to gs://bucket_1990/benchmark_results/shuffle_results\n",
      "24/01/19 11:37:51 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://bucket_1990/benchmark_results/' directory.\n",
      "24/01/19 11:37:51 INFO com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_delete. latencyMs=399; previousMaxLatencyMs=0; operationCount=1; context=gs://bucket_1990/benchmark_results/shuffle_results\n",
      "24/01/19 11:37:51 INFO com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=179; previousMaxLatencyMs=162; operationCount=2; context=gs://bucket_1990/benchmark_results/shuffle_results/_temporary/0\n",
      "24/01/19 11:37:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://bucket_1990/benchmark_results/shuffle_results/' directory.\n",
      "24/01/19 11:37:54 INFO com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=234; previousMaxLatencyMs=0; operationCount=1; context=gs://bucket_1990/benchmark_results/shuffle_results/_SUCCESS\n",
      "24/01/19 11:37:54 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@461e3540{HTTP/1.1, (http/1.1)}{0.0.0.0:0}\n",
      "Job [c18ad85312d441ba9d175d9ea1cbe3a9] finished successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "driverControlFilesUri: gs://dataproc-staging-europe-west6-592252024140-2eafoyog/google-cloud-dataproc-metainfo/33b9567d-8933-4b45-b085-d8e6f9dd801d/jobs/c18ad85312d441ba9d175d9ea1cbe3a9/\n",
      "driverOutputResourceUri: gs://dataproc-staging-europe-west6-592252024140-2eafoyog/google-cloud-dataproc-metainfo/33b9567d-8933-4b45-b085-d8e6f9dd801d/jobs/c18ad85312d441ba9d175d9ea1cbe3a9/driveroutput\n",
      "jobUuid: a9806a9c-e378-3d10-b826-d43e1198124c\n",
      "placement:\n",
      "  clusterName: nombre-del-cluster\n",
      "  clusterUuid: 33b9567d-8933-4b45-b085-d8e6f9dd801d\n",
      "pysparkJob:\n",
      "  args:\n",
      "  - gs://bucket_1990/Data/datos_generados.csv\n",
      "  - -n\n",
      "  - \"'shuffle-benchmark'\"\n",
      "  - -r\n",
      "  - '200'\n",
      "  - -o\n",
      "  - gs://bucket_1990/benchmark_results/shuffle_results\n",
      "  mainPythonFileUri: gs://bucket_1990/benchmark-shuffle.py\n",
      "reference:\n",
      "  jobId: c18ad85312d441ba9d175d9ea1cbe3a9\n",
      "  projectId: practica-3-411314\n",
      "status:\n",
      "  state: DONE\n",
      "  stateStartTime: '2024-01-19T11:37:58.817563Z'\n",
      "statusHistory:\n",
      "- state: PENDING\n",
      "  stateStartTime: '2024-01-19T11:36:49.203248Z'\n",
      "- state: SETUP_DONE\n",
      "  stateStartTime: '2024-01-19T11:36:49.252662Z'\n",
      "- details: Agent reported job success\n",
      "  state: RUNNING\n",
      "  stateStartTime: '2024-01-19T11:36:49.475919Z'\n",
      "yarnApplications:\n",
      "- name: \"'shuffle-benchmark'\"\n",
      "  progress: 1.0\n",
      "  state: FINISHED\n",
      "  trackingUrl: http://nombre-del-cluster-m:8088/proxy/application_1705477825542_0006/\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc jobs submit pyspark \\\n",
    "  --cluster=nombre-del-cluster \\\n",
    "  --region=europe-west6 \\\n",
    "  gs://bucket_1990/benchmark-shuffle.py \\\n",
    "  -- gs://bucket_1990/Data/datos_generados.csv -n 'shuffle-benchmark' -r 200 -o gs://bucket_1990/benchmark_results/shuffle_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59292681-3d09-47f3-a319-e06ebd4bf408",
   "metadata": {},
   "source": [
    "### Resultados de la ejecución:\n",
    "- **Group By test time** = 15.213665729999775 seconds: Indica el tiempo que ha tomado ejecutar la operación de agrupación (Group By). En este caso, la operación de agrupación ha tomado aproximadamente 15.21 segundos.\n",
    " \n",
    "- **Repartition test time** = 8.370773898001062 seconds (200 partitions): Muestra el tiempo de ejecución de la operación de repartición (Repartition). Además, indica que durante esta prueba se utilizó un total de 200 particiones. En este caso, a la operación de repartición le ha llevado aproximadamente 8.37 segundos\n",
    "\n",
    "- **Inner join test time** = 14.873300867009675 seconds: Indica el tiempo que tomó ejecutar la operación de unión interna (Inner Join). En este caso, la operación de unión interna ha tardado aproximadamente 14.87 segundos.\n",
    "\n",
    "- **Broadcast inner join time** = 6.686923230998218 seconds: Representa el tiempo de ejecución de la operación de unión interna mediante difusión (Broadcast Inner Join). Aquí, la operación de unión interna mediante difusión ha tomado aproximadamente 6.69 segundos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6adb5a-7c86-4b0f-93f2-51de63c2e5f5",
   "metadata": {},
   "source": [
    "## Ejecución del Benchmark en Dataproc: benchmark-cpu.py\n",
    "El benchmark de CPU tiene como objetivo evaluar operaciones de PySpark que están principalmente vinculadas a la CPU. A diferencia de otras pruebas, este benchmark se centra en evaluar la velocidad de la CPU y la eficiencia de las tareas, sin considerar E/S de disco o red significativas. Las operaciones de prueba incluyen:\n",
    "\n",
    "- SHA-512 hashing de una cadena.\n",
    "- Estimación de Pi con muestras aleatorias y una función Python definida por el usuario.\n",
    "- Estimación de Pi con muestras aleatorias y solo funciones nativas de Spark.\n",
    "\n",
    "Cada operación se cronometra de forma independiente.\n",
    "\n",
    "Los resultados proporcionan información valiosa sobre el rendimiento de las operaciones de CPU en PySpark, y los tiempos se pueden comparar entre ejecuciones para evaluar la eficiencia del entorno PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d6587eb-b8ee-429c-9759-1aa7cf72efca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: true\n",
      "driverControlFilesUri: gs://dataproc-staging-europe-west6-592252024140-2eafoyog/google-cloud-dataproc-metainfo/33b9567d-8933-4b45-b085-d8e6f9dd801d/jobs/dbe21945b5014deb89450c61b3fd0613/\n",
      "driverOutputResourceUri: gs://dataproc-staging-europe-west6-592252024140-2eafoyog/google-cloud-dataproc-metainfo/33b9567d-8933-4b45-b085-d8e6f9dd801d/jobs/dbe21945b5014deb89450c61b3fd0613/driveroutput\n",
      "jobUuid: 9a322f01-84b1-3c8b-8c2a-3a1345af6c03\n",
      "placement:\n",
      "  clusterName: nombre-del-cluster\n",
      "  clusterUuid: 33b9567d-8933-4b45-b085-d8e6f9dd801d\n",
      "pysparkJob:\n",
      "  args:\n",
      "  - gs://bucket_1990/Data/datos_generados.csv\n",
      "  - -s\n",
      "  - '5000000000'\n",
      "  - -p\n",
      "  - '10'\n",
      "  - -n\n",
      "  - \"'cpu-benchmark'\"\n",
      "  - -o\n",
      "  - gs://bucket_1990/benchmark_results/cpu_results\n",
      "  mainPythonFileUri: gs://bucket_1990/benchmark-cpu.py\n",
      "reference:\n",
      "  jobId: dbe21945b5014deb89450c61b3fd0613\n",
      "  projectId: practica-3-411314\n",
      "status:\n",
      "  state: DONE\n",
      "  stateStartTime: '2024-01-19T11:59:24.239727Z'\n",
      "statusHistory:\n",
      "- state: PENDING\n",
      "  stateStartTime: '2024-01-19T11:43:17.779752Z'\n",
      "- state: SETUP_DONE\n",
      "  stateStartTime: '2024-01-19T11:43:17.824478Z'\n",
      "- details: Agent reported job success\n",
      "  state: RUNNING\n",
      "  stateStartTime: '2024-01-19T11:43:18.096505Z'\n",
      "yarnApplications:\n",
      "- name: \"'cpu-benchmark'\"\n",
      "  progress: 1.0\n",
      "  state: FINISHED\n",
      "  trackingUrl: http://nombre-del-cluster-m:8088/proxy/application_1705477825542_0007/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [dbe21945b5014deb89450c61b3fd0613] submitted.\n",
      "Waiting for job output...\n",
      "24/01/19 11:43:22 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "24/01/19 11:43:22 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "24/01/19 11:43:22 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/01/19 11:43:22 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "24/01/19 11:43:22 INFO org.sparkproject.jetty.util.log: Logging initialized @3876ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "24/01/19 11:43:22 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_392-b08\n",
      "24/01/19 11:43:22 INFO org.sparkproject.jetty.server.Server: Started @4014ms\n",
      "24/01/19 11:43:22 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@60110b72{HTTP/1.1, (http/1.1)}{0.0.0.0:35987}\n",
      "24/01/19 11:43:23 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at nombre-del-cluster-m/10.172.0.3:8032\n",
      "24/01/19 11:43:23 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at nombre-del-cluster-m/10.172.0.3:10200\n",
      "24/01/19 11:43:25 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\n",
      "24/01/19 11:43:25 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "24/01/19 11:43:25 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1705477825542_0007\n",
      "24/01/19 11:43:26 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at nombre-del-cluster-m/10.172.0.3:8030\n",
      "24/01/19 11:43:28 INFO com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=216; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-europe-west6-592252024140-wiwqtlkk/33b9567d-8933-4b45-b085-d8e6f9dd801d/spark-job-history\n",
      "24/01/19 11:43:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
      "24/01/19 11:43:30 INFO __main__: **********************************************************************\n",
      "24/01/19 11:43:30 INFO __main__: \n",
      "24/01/19 11:43:30 INFO __main__: Benchmarking PySpark's CPU throughput using input data at gs://bucket_1990/Data/datos_generados.csv\n",
      "24/01/19 11:43:30 INFO __main__: \n",
      "24/01/19 11:43:30 INFO __main__: **********************************************************************\n",
      "24/01/19 11:43:33 INFO __main__: ****************************************************************\n",
      "24/01/19 11:43:33 INFO __main__: Starting benchmark test calculatng SHA-512 hashes\n",
      "24/01/19 11:43:47 INFO __main__: ****************************************************************\n",
      "24/01/19 11:43:47 INFO __main__: Starting benchmark test calculatng Pi with 5,000,000,000 samples\n",
      "24/01/19 11:58:00 INFO __main__: ****************************************************************\n",
      "24/01/19 11:58:00 INFO __main__: Starting benchmark test calculatng Pi via dataframe manipulations with 5,000,000,000 samples\n",
      "24/01/19 11:59:15 INFO __main__: ****************************************************************************\n",
      "24/01/19 11:59:15 INFO __main__:     RESULTS    RESULTS    RESULTS    RESULTS    RESULTS    RESULTS\n",
      "24/01/19 11:59:15 INFO __main__:     Test Run = 'cpu-benchmark'\n",
      "24/01/19 11:59:15 INFO __main__: \n",
      "24/01/19 11:59:15 INFO __main__: SHA-512 benchmark time                 = 14.611094254010823 seconds for 1,000,000 hashes\n",
      "24/01/19 11:59:15 INFO __main__: Calculate Pi benchmark                 = 852.4776166210067 seconds with pi = 3.1415917352, samples = 5,000,000,000\n",
      "24/01/19 11:59:15 INFO __main__: Calculate Pi benchmark using dataframe = 74.6676025170018 seconds with pi = 3.1415522944, samples = 5,000,000,000\n",
      "24/01/19 11:59:15 INFO __main__: \n",
      "24/01/19 11:59:15 INFO __main__: ****************************************************************************\n",
      "24/01/19 11:59:15 INFO __main__: \n",
      "24/01/19 11:59:15 INFO __main__: Writing results to gs://bucket_1990/benchmark_results/cpu_results\n",
      "24/01/19 11:59:15 INFO com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_delete. latencyMs=226; previousMaxLatencyMs=0; operationCount=1; context=gs://bucket_1990/benchmark_results/cpu_results\n",
      "24/01/19 11:59:15 INFO com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=192; previousMaxLatencyMs=116; operationCount=2; context=gs://bucket_1990/benchmark_results/cpu_results/_temporary/0\n",
      "24/01/19 11:59:21 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://bucket_1990/benchmark_results/cpu_results/' directory.\n",
      "24/01/19 11:59:21 INFO com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_delete. latencyMs=340; previousMaxLatencyMs=226; operationCount=2; context=gs://bucket_1990/benchmark_results/cpu_results/_temporary\n",
      "24/01/19 11:59:21 INFO com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=242; previousMaxLatencyMs=0; operationCount=1; context=gs://bucket_1990/benchmark_results/cpu_results/_SUCCESS\n",
      "24/01/19 11:59:21 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@60110b72{HTTP/1.1, (http/1.1)}{0.0.0.0:0}\n",
      "Job [dbe21945b5014deb89450c61b3fd0613] finished successfully.\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc jobs submit pyspark \\\n",
    "    --cluster=nombre-del-cluster \\\n",
    "    --region=europe-west6 \\\n",
    "    gs://bucket_1990/benchmark-cpu.py \\\n",
    "    -- gs://bucket_1990/Data/datos_generados.csv -s 5000000000 -p 10 -n 'cpu-benchmark' -o gs://bucket_1990/benchmark_results/cpu_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be0a149-bba4-4fa3-be9e-82fbc7322095",
   "metadata": {},
   "source": [
    "### Resultados de la ejecución\n",
    "Los resultados del benchmark de CPU muestran el tiempo que tardó cada operación en ejecutarse:\n",
    "\n",
    "- **SHA-512 benchmark time**: Este resultado indica el tiempo que le ha llevado calcular 100,000 hashes SHA-512. En este caso, ha sido de aproximadamente 14.61 segundos. Esta prueba mide la velocidad de procesamiento para operaciones intensivas en CPU, como la generación de hashes.\n",
    "\n",
    "- **Calculate Pi benchmark**: Para esta prueba, se han tomado 5 mil millones de muestras para estimar el valor de Pi. El resultado muestra que ha tardado alrededor de 852.47 segundos (algo más de 14 minutos) realizar esta operación.\n",
    "\n",
    "- **Calculate Pi benchmark using dataframe**: Similar a la prueba anterior, esta vez, la estimación de Pi se ha realizado utilizando manipulaciones de DataFrame. Ha tomado aproximadamente 74.67 segundos. Esto proporciona una comparación entre el rendimiento de las operaciones puras de PySpark y aquellas que involucran manipulaciones de DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa9477fa-2b02-4c6e-a5e3-ac1fcbbe27ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusterName: nombre-del-cluster\n",
      "clusterUuid: 33b9567d-8933-4b45-b085-d8e6f9dd801d\n",
      "config:\n",
      "  configBucket: dataproc-staging-europe-west6-592252024140-2eafoyog\n",
      "  endpointConfig: {}\n",
      "  gceClusterConfig:\n",
      "    internalIpOnly: false\n",
      "    networkUri: https://www.googleapis.com/compute/v1/projects/practica-3-411314/global/networks/default\n",
      "    serviceAccountScopes:\n",
      "    - https://www.googleapis.com/auth/bigquery\n",
      "    - https://www.googleapis.com/auth/bigtable.admin.table\n",
      "    - https://www.googleapis.com/auth/bigtable.data\n",
      "    - https://www.googleapis.com/auth/cloud.useraccounts.readonly\n",
      "    - https://www.googleapis.com/auth/devstorage.full_control\n",
      "    - https://www.googleapis.com/auth/devstorage.read_write\n",
      "    - https://www.googleapis.com/auth/logging.write\n",
      "    - https://www.googleapis.com/auth/monitoring.write\n",
      "    zoneUri: https://www.googleapis.com/compute/v1/projects/practica-3-411314/zones/europe-west6-c\n",
      "  masterConfig:\n",
      "    diskConfig:\n",
      "      bootDiskSizeGb: 1000\n",
      "      bootDiskType: pd-standard\n",
      "    imageUri: https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-2-0-deb10-20231208-035100-rc01\n",
      "    instanceNames:\n",
      "    - nombre-del-cluster-m\n",
      "    machineTypeUri: https://www.googleapis.com/compute/v1/projects/practica-3-411314/zones/europe-west6-c/machineTypes/n1-standard-4\n",
      "    minCpuPlatform: AUTOMATIC\n",
      "    numInstances: 1\n",
      "    preemptibility: NON_PREEMPTIBLE\n",
      "  softwareConfig:\n",
      "    imageVersion: 2.0.87-debian10\n",
      "    properties:\n",
      "      capacity-scheduler:yarn.scheduler.capacity.root.default.ordering-policy: fair\n",
      "      core:fs.gs.block.size: '134217728'\n",
      "      core:fs.gs.metadata.cache.enable: 'false'\n",
      "      core:hadoop.ssl.enabled.protocols: TLSv1,TLSv1.1,TLSv1.2\n",
      "      distcp:mapreduce.map.java.opts: -Xmx768m\n",
      "      distcp:mapreduce.map.memory.mb: '1024'\n",
      "      distcp:mapreduce.reduce.java.opts: -Xmx768m\n",
      "      distcp:mapreduce.reduce.memory.mb: '1024'\n",
      "      hadoop-env:HADOOP_DATANODE_OPTS: -Xmx512m\n",
      "      hdfs:dfs.datanode.address: 0.0.0.0:9866\n",
      "      hdfs:dfs.datanode.http.address: 0.0.0.0:9864\n",
      "      hdfs:dfs.datanode.https.address: 0.0.0.0:9865\n",
      "      hdfs:dfs.datanode.ipc.address: 0.0.0.0:9867\n",
      "      hdfs:dfs.namenode.handler.count: '20'\n",
      "      hdfs:dfs.namenode.http-address: 0.0.0.0:9870\n",
      "      hdfs:dfs.namenode.https-address: 0.0.0.0:9871\n",
      "      hdfs:dfs.namenode.lifeline.rpc-address: nombre-del-cluster-m:8050\n",
      "      hdfs:dfs.namenode.secondary.http-address: 0.0.0.0:9868\n",
      "      hdfs:dfs.namenode.secondary.https-address: 0.0.0.0:9869\n",
      "      hdfs:dfs.namenode.service.handler.count: '10'\n",
      "      hdfs:dfs.namenode.servicerpc-address: nombre-del-cluster-m:8051\n",
      "      hive:hive.fetch.task.conversion: none\n",
      "      mapred-env:HADOOP_JOB_HISTORYSERVER_HEAPSIZE: '3840'\n",
      "      mapred:mapreduce.job.maps: '21'\n",
      "      mapred:mapreduce.job.reduce.slowstart.completedmaps: '0.95'\n",
      "      mapred:mapreduce.job.reduces: '7'\n",
      "      mapred:mapreduce.jobhistory.recovery.store.class: org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService\n",
      "      mapred:mapreduce.map.cpu.vcores: '1'\n",
      "      mapred:mapreduce.map.java.opts: -Xmx2524m\n",
      "      mapred:mapreduce.map.memory.mb: '3156'\n",
      "      mapred:mapreduce.reduce.cpu.vcores: '1'\n",
      "      mapred:mapreduce.reduce.java.opts: -Xmx2524m\n",
      "      mapred:mapreduce.reduce.memory.mb: '3156'\n",
      "      mapred:mapreduce.task.io.sort.mb: '256'\n",
      "      mapred:yarn.app.mapreduce.am.command-opts: -Xmx2524m\n",
      "      mapred:yarn.app.mapreduce.am.resource.cpu-vcores: '1'\n",
      "      mapred:yarn.app.mapreduce.am.resource.mb: '3156'\n",
      "      spark-env:SPARK_DAEMON_MEMORY: 3840m\n",
      "      spark:spark.driver.maxResultSize: 1920m\n",
      "      spark:spark.driver.memory: 3840m\n",
      "      spark:spark.executor.cores: '2'\n",
      "      spark:spark.executor.instances: '2'\n",
      "      spark:spark.executor.memory: 5739m\n",
      "      spark:spark.executorEnv.OPENBLAS_NUM_THREADS: '1'\n",
      "      spark:spark.scheduler.mode: FAIR\n",
      "      spark:spark.sql.cbo.enabled: 'true'\n",
      "      spark:spark.ui.port: '0'\n",
      "      spark:spark.yarn.am.memory: 640m\n",
      "      yarn-env:YARN_NODEMANAGER_HEAPSIZE: '1536'\n",
      "      yarn-env:YARN_RESOURCEMANAGER_HEAPSIZE: '3840'\n",
      "      yarn-env:YARN_TIMELINESERVER_HEAPSIZE: '3840'\n",
      "      yarn:yarn.nodemanager.address: 0.0.0.0:8026\n",
      "      yarn:yarn.nodemanager.resource.cpu-vcores: '4'\n",
      "      yarn:yarn.nodemanager.resource.memory-mb: '12624'\n",
      "      yarn:yarn.resourcemanager.decommissioning-nodes-watcher.decommission-if-no-shuffle-data: 'true'\n",
      "      yarn:yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs: '86400'\n",
      "      yarn:yarn.scheduler.maximum-allocation-mb: '12624'\n",
      "      yarn:yarn.scheduler.minimum-allocation-mb: '1'\n",
      "  tempBucket: dataproc-temp-europe-west6-592252024140-wiwqtlkk\n",
      "  workerConfig:\n",
      "    diskConfig:\n",
      "      bootDiskSizeGb: 1000\n",
      "      bootDiskType: pd-standard\n",
      "    imageUri: https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-2-0-deb10-20231208-035100-rc01\n",
      "    instanceNames:\n",
      "    - nombre-del-cluster-w-0\n",
      "    - nombre-del-cluster-w-1\n",
      "    machineTypeUri: https://www.googleapis.com/compute/v1/projects/practica-3-411314/zones/europe-west6-c/machineTypes/n1-standard-4\n",
      "    minCpuPlatform: AUTOMATIC\n",
      "    numInstances: 2\n",
      "    preemptibility: NON_PREEMPTIBLE\n",
      "labels:\n",
      "  goog-dataproc-autozone: enabled\n",
      "  goog-dataproc-cluster-name: nombre-del-cluster\n",
      "  goog-dataproc-cluster-uuid: 33b9567d-8933-4b45-b085-d8e6f9dd801d\n",
      "  goog-dataproc-location: europe-west6\n",
      "metrics:\n",
      "  hdfsMetrics:\n",
      "    dfs-blocks-corrupt: '0'\n",
      "    dfs-blocks-default-replication-factor: '2'\n",
      "    dfs-blocks-missing: '0'\n",
      "    dfs-blocks-missing-repl-one: '0'\n",
      "    dfs-blocks-pending-deletion: '0'\n",
      "    dfs-blocks-under-replication: '0'\n",
      "    dfs-capacity-present: '2001249259520'\n",
      "    dfs-capacity-remaining: '2001249185792'\n",
      "    dfs-capacity-total: '2113237753856'\n",
      "    dfs-capacity-used: '73728'\n",
      "    dfs-nodes-decommissioned: '0'\n",
      "    dfs-nodes-decommissioning: '0'\n",
      "    dfs-nodes-running: '2'\n",
      "  yarnMetrics:\n",
      "    yarn-apps-completed: '7'\n",
      "    yarn-apps-failed: '0'\n",
      "    yarn-apps-killed: '0'\n",
      "    yarn-apps-pending: '0'\n",
      "    yarn-apps-running: '0'\n",
      "    yarn-apps-submitted: '7'\n",
      "    yarn-containers-allocated: '0'\n",
      "    yarn-containers-pending: '0'\n",
      "    yarn-containers-reserved: '0'\n",
      "    yarn-memory-mb-allocated: '0'\n",
      "    yarn-memory-mb-available: '25248'\n",
      "    yarn-memory-mb-pending: '0'\n",
      "    yarn-memory-mb-reserved: '0'\n",
      "    yarn-memory-mb-total: '25248'\n",
      "    yarn-nodes-active: '2'\n",
      "    yarn-nodes-decommissioned: '0'\n",
      "    yarn-nodes-decommissioning: '0'\n",
      "    yarn-nodes-lost: '0'\n",
      "    yarn-nodes-new: '0'\n",
      "    yarn-nodes-rebooted: '0'\n",
      "    yarn-nodes-shutdown: '0'\n",
      "    yarn-nodes-unhealthy: '0'\n",
      "    yarn-vcores-allocated: '0'\n",
      "    yarn-vcores-available: '8'\n",
      "    yarn-vcores-pending: '0'\n",
      "    yarn-vcores-reserved: '0'\n",
      "    yarn-vcores-total: '8'\n",
      "projectId: practica-3-411314\n",
      "status:\n",
      "  state: RUNNING\n",
      "  stateStartTime: '2024-01-17T07:51:08.086661Z'\n",
      "statusHistory:\n",
      "- state: CREATING\n",
      "  stateStartTime: '2024-01-17T07:49:01.173137Z'\n"
     ]
    }
   ],
   "source": [
    "# Información sobre el cluster\n",
    "!gcloud dataproc clusters describe nombre-del-cluster --region=europe-west6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc1d00b",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "El tiempo de todas las pruebas es mayor que los valores de referencia que se indican en el siguiente enlace https://openbenchmarking.org/test/pts/spark&eval=8dbf91eee81b8c1295c3d871d534bd6055fa787b#metrics (1000000 rows y 100 partitions), lo que indica un bajo rendimiento del cluster creado. Sería interesante ajustar la configuración del clúster, como la cantidad de nodos o tipo de máquinas virtuales, para mejorar el rendimiento en las pruebas mencionadas, pero debido a las limitaciones de la cuenta esto no ha sido posible.\n",
    "\t\n",
    "Como conclusión final, la implementación de benchmarks en el clúster de GCS proporciona una base valiosa para la optimización continua y la toma de decisiones informadas para garantizar un rendimiento eficiente y rentable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f37ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
